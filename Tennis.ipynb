{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "import imageio\n",
    "from collections import deque\n",
    "import progressbar as pb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "BUFFER_SIZE = int(1e6)               # max size of memory buffer\n",
    "N_EPISODES = 20000                   # number of training episodes\n",
    "EPISODE_MAX_T = 10000                # max timesteps each episode\n",
    "BATCH_SIZE = 1024                    # size of batch\n",
    "SAVE_INTERVAL = 1000                 # how many episodes to save policy\n",
    "HIDDEN_NETWORK = [256, 256, 128]     # Structure of the hidden layer of the networks\n",
    "LR_ACTOR = 1e-3                      # Learning rate of actor networks\n",
    "LR_CRITIC = 1e-3                     # Learning rate of critic networks\n",
    "ALLOW_BN = False                     # Allow Batch Normalization\n",
    "\n",
    "NOISE = 2                            # amplitude of OU noise\n",
    "NOISE_REDUCTION = 0.99995            # this slowly decreases to 0\n",
    "EPISODE_PER_UPDATE = 1               # how many episodes before update\n",
    "\n",
    "PRINT_EVERY = 100                    # how many episodes to print the medium scores\n",
    "MOVING_AVERAGE_WINDOW = 100          # Size of moving average window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "model_path = os.path.join(model_dir, 'checkpoint.pth')\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=948):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env(env, brain_name, train_mode, verbose = False):\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    \n",
    "    if verbose:\n",
    "        # number of agents in the environment\n",
    "        print('Number of agents:', num_agents)\n",
    "        \n",
    "        # size of each action\n",
    "        print('Size of each action:', action_size)\n",
    "\n",
    "        # number of actions\n",
    "        action_size = brain.vector_action_space_size\n",
    "        print('Number of actions:', action_size)\n",
    "\n",
    "        # examine the state space \n",
    "        states = env_info.vector_observations\n",
    "        state_size = states[0].shape[1]\n",
    "        print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "        print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "    return env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_env_info(env_info):\n",
    "    states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    \n",
    "    return states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, moving_average_window=MOVING_AVERAGE_WINDOW, goal_threshold=0.5):\n",
    "    moving_average = []\n",
    "    moving_average_deque = deque(maxlen=MOVING_AVERAGE_WINDOW)\n",
    "    for i in range(len(scores)):\n",
    "        moving_average_deque.append(scores[i])\n",
    "        moving_average.append(np.mean(moving_average_deque))\n",
    "    \n",
    "    goal_line_data = np.array([goal_threshold for i in np.arange(1, len(scores)+1)])\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(np.arange(1, len(scores)+1), scores, color='b', label='Scores')\n",
    "    plt.plot(np.arange(1, len(scores)+1), moving_average, color='r', label='Average (last 100)')\n",
    "    plt.plot(np.arange(1, len(scores)+1), goal_line_data, 'r--', label='Goal threshold') \n",
    "    \n",
    "    #Here we got little help from:\n",
    "    #https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_game(maddpg, n_episodes = 5, max_t = 200, random_seed = 0):\n",
    "    agents = maddpg.maddpg_agent\n",
    "    num_agents = len(agents)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # load the weights from files\n",
    "    for i in range(num_agents):\n",
    "        agents[i].actor.load_state_dict(checkpoint['agent_{}'.format(str(i+1))]['actor_params'])\n",
    "        agents[i].critic.load_state_dict(checkpoint['agent_{}'.format(str(i+1))]['critic_params'])\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        env_info = reset_env(env, brain_name, train_mode=False)\n",
    "        states, rewards, dones = get_data_from_env_info(env_info)\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = maddpg.act(states)\n",
    "            actions_array = torch.stack(actions).detach().cpu().numpy()\n",
    "            \n",
    "            env_info = env.step(actions_array)[brain_name]\n",
    "            states, rewards, dones = get_data_from_env_info(env_info)\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        print(\"Episode {0} winner agent reward: {1}\".format(i_episode+1, max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, maddpg):\n",
    "    seeding()\n",
    "    \n",
    "    buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed=1)\n",
    "    \n",
    "    episode_scores = []\n",
    "    episode_scores_deque = deque(maxlen=MOVING_AVERAGE_WINDOW)\n",
    "\n",
    "    # show progressbar\n",
    "    #widget = ['episode: ', pb.Counter(),'/',str(N_EPISODES),' ', \n",
    "    #          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "    #timer = pb.ProgressBar(widgets=widget, maxval=N_EPISODES).start()\n",
    "\n",
    "    noise = NOISE\n",
    "    \n",
    "    for episode in range(N_EPISODES):\n",
    "        reward_this_episode = np.zeros(2)\n",
    "\n",
    "        env_info = reset_env(env, brain_name, train_mode=True)\n",
    "        states, _0, _1 = get_data_from_env_info(env_info)\n",
    "        \n",
    "        save_info = ((episode+1) % SAVE_INTERVAL) == 0\n",
    "        \n",
    "        for episode_t in range(EPISODE_MAX_T):\n",
    "            actions = maddpg.act(states, noise=noise)\n",
    "            noise *= NOISE_REDUCTION\n",
    "            actions_array = torch.stack(actions).detach().cpu().numpy()\n",
    "            \n",
    "            env_info = env.step(actions_array)[brain_name]\n",
    "            \n",
    "            next_states, rewards, dones = get_data_from_env_info(env_info)\n",
    "            \n",
    "            states_full = np.concatenate([states[0], states[1]])\n",
    "            next_states_full = np.concatenate([next_states[0], next_states[1]])\n",
    "            \n",
    "            experience = (states, states_full, actions_array, rewards, next_states, next_states_full, dones)\n",
    "            buffer.add(*experience)\n",
    "            \n",
    "            reward_this_episode += rewards\n",
    "            \n",
    "            states, states_full = next_states, next_states_full\n",
    "        \n",
    "            if (np.any(dones)):\n",
    "                break\n",
    "                \n",
    "        # update once after every EPISODE_PER_UPDATE\n",
    "        if len(buffer) > BATCH_SIZE and (episode+1) % EPISODE_PER_UPDATE == 0:\n",
    "            for a_i in range(2):\n",
    "                samples = buffer.sample()\n",
    "                maddpg.update(samples, a_i)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "        \n",
    "        efetive_episode_score = max(reward_this_episode[0], reward_this_episode[1])\n",
    "        episode_scores.append(efetive_episode_score)\n",
    "        episode_scores_deque.append(efetive_episode_score)\n",
    "        moving_average = np.mean(episode_scores_deque)\n",
    "        \n",
    "        if (episode+1) % PRINT_EVERY == 0 or episode == N_EPISODES-1:\n",
    "            print(\"Episode {0}\\t Moving average(last {1}): {2:.4f}\"\n",
    "                      .format(episode+1, MOVING_AVERAGE_WINDOW, moving_average, noise))\n",
    "            \n",
    "        #timer.update(episode)     \n",
    "\n",
    "        solved = False;\n",
    "        if moving_average >= 0.5:\n",
    "            solved = True\n",
    "            save_info = True\n",
    "            print(\"Solved in {0} episodes! Last moving average: {1}\"\n",
    "                 .format(episode+1, moving_average))\n",
    "            \n",
    "        #saving model\n",
    "        save_dict = { 'episodes': episode, 'agent_1': {}, 'agent_2': {} }\n",
    "        if save_info:\n",
    "            for i in range(2):\n",
    "                save_dict['agent_{}'.format(str(i+1))] = {\n",
    "                             'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "\n",
    "                torch.save(save_dict, \n",
    "                           model_path)\n",
    "                \n",
    "        if solved:\n",
    "            break;\n",
    "            \n",
    "    #timer.finish()\n",
    "    return episode_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = MADDPG(\n",
    "    hidden_network=HIDDEN_NETWORK,\n",
    "    lr_actor=LR_ACTOR,\n",
    "    lr_critic=LR_CRITIC,\n",
    "    allow_bn=ALLOW_BN,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Moving average(last 100): 0.0060\n",
      "Episode 200\t Moving average(last 100): 0.0019\n",
      "Episode 300\t Moving average(last 100): 0.0027\n",
      "Episode 400\t Moving average(last 100): 0.0010\n",
      "Episode 500\t Moving average(last 100): 0.0039\n",
      "Episode 600\t Moving average(last 100): 0.0040\n",
      "Episode 700\t Moving average(last 100): 0.0199\n",
      "Episode 800\t Moving average(last 100): 0.0050\n",
      "Episode 900\t Moving average(last 100): 0.0069\n",
      "Episode 1000\t Moving average(last 100): 0.0150\n",
      "Episode 1100\t Moving average(last 100): 0.0280\n",
      "Episode 1200\t Moving average(last 100): 0.0330\n",
      "Episode 1300\t Moving average(last 100): 0.0240\n",
      "Episode 1400\t Moving average(last 100): 0.0110\n",
      "Episode 1500\t Moving average(last 100): 0.0240\n",
      "Episode 1600\t Moving average(last 100): 0.0350\n",
      "Episode 1700\t Moving average(last 100): 0.0480\n",
      "Episode 1800\t Moving average(last 100): 0.0490\n",
      "Episode 1900\t Moving average(last 100): 0.0450\n",
      "Episode 2000\t Moving average(last 100): 0.0675\n",
      "Episode 2100\t Moving average(last 100): 0.0739\n",
      "Episode 2200\t Moving average(last 100): 0.0698\n",
      "Episode 2300\t Moving average(last 100): 0.0852\n",
      "Episode 2400\t Moving average(last 100): 0.0846\n",
      "Episode 2500\t Moving average(last 100): 0.0720\n",
      "Episode 2600\t Moving average(last 100): 0.0400\n",
      "Episode 2700\t Moving average(last 100): 0.0539\n",
      "Episode 2800\t Moving average(last 100): 0.0560\n",
      "Episode 2900\t Moving average(last 100): 0.1216\n",
      "Episode 3000\t Moving average(last 100): 0.1356\n",
      "Episode 3100\t Moving average(last 100): 0.1442\n",
      "Episode 3200\t Moving average(last 100): 0.1858\n",
      "Episode 3300\t Moving average(last 100): 0.3527\n",
      "Episode 3400\t Moving average(last 100): 0.3590\n",
      "Solved in 3447 episodes! Last moving average: 0.5170000077039003\n"
     ]
    }
   ],
   "source": [
    "scores = main(env, maddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAEGCAYAAADxFTYDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hVVdr38e+dnpgQIIQmJQgkECBSI2IYuwwCFnAGUMEylnFkEH3UUXmtozPjPKIjOthGLIyj6INj7w1BbJHeCQKCJEgJSZAQUtb7xz6RnPQAJwnk97muc52991pnn/vslH2ftddey5xziIiIiJQKaugAREREpHFRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPgJaegA6qpVq1YuISGhocMQETmifPfddzucc/ENHYccGY645CAhIYH09PSGDkNE5IhiZpsaOgY5cuiygoiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+ApYcmFlHM/vUzFaZ2Qozu66SOqeYWY6ZLfY97ghUPCIiIlI7IQHcdxHwP865hWYWA3xnZh8651aWqzfPOTcygHGIiIhIHQSs5cA5l+mcW+hbzgNWAccG6v1ERETk8KiXPgdmlgD0A76upPhEM1tiZu+aWa8qXn+VmaWbWfr27dsDGKmIiIgEPDkws2hgDjDFOZdbrngh0Nk5dzzwCPBaZftwzj3pnBvonBsYHx8f2IBFRESauIAmB2YWipcYvOCce7V8uXMu1zm3x7f8DhBqZq0CGZOIiIhUL5B3KxjwNLDKOfdgFXXa+uphZqm+eHYGKiYRERGpWSDvVjgJmAAsM7PFvm23AZ0AnHOPAxcA15hZEZAPjHPOuQDGJCIiIjUIWHLgnJsPWA11HgUeDVQMIiIiUncaIVFERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oOREQaOat2IHqRw0/JgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIHyUHIiIi4kfJgYiIiPhRciAiIiJ+lByIiIiIn4AlB2bW0cw+NbNVZrbCzK6rpI6Z2XQzyzCzpWbWP1DxiIiISO2EBHDfRcD/OOcWmlkM8J2ZfeicW1mmznCgu+9xAvCY71lEREQaSMBaDpxzmc65hb7lPGAVcGy5aucCzzvPV0BzM2sXqJhERESkZvXS58DMEoB+wNflio4FNpdZ30LFBEJERETqUcCTAzOLBuYAU5xzueWLK3mJq2QfV5lZupmlb9++PRBhioiIiE9AkwMzC8VLDF5wzr1aSZUtQMcy6x2AreUrOeeedM4NdM4NjI+PD0ywIiIiAgT2bgUDngZWOecerKLaG8BE310Lg4Ec51xmoGISERGRmgXyboWTgAnAMjNb7Nt2G9AJwDn3OPAOcDaQAewFLgtgPCIiIlILAUsOnHPzqbxPQdk6Drg2UDGIiIhI3WmERBEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfGj5EBERET8KDkQERERP0oORERExI+SAxEREfET0tABiIhIw/juu+9ah4SE/Avojb4sNjUlwPKioqIrBgwY8FP5QiUHIiJNVEhIyL/atm3bMz4+PjsoKMg1dDxSf0pKSmz79u3JWVlZ/wLOKV+uTFFEpOnqHR8fn6vEoOkJCgpy8fHxOXitRhXL6zkeERFpPIKUGDRdvp99pXmAkgMRERHxo+RAREQa1PPPP9/czAYsWrQooqFjqY0vvvgicuzYsZ0Bpk+fHjdx4sROdd3HmjVrwh5//PGWVZUPHTq0e0xMTN9TTz21W9ntq1evDktJSenRuXPn3iNGjDhu3759BpCfn28jRow4rlOnTr1TUlJ6rFmzJgzgm2++iRwzZkxCXeNTciAiIg3qpZdeatm/f/89s2bNqvJkWRdFRUWHYzdVuvfee9tNmTKlQg//uli3bl347Nmzq/y8N954Y9YTTzyxofz2G264ocOkSZO2bdq0aXlsbGzRww8/3Arg4YcfbhUbG1v0ww8/LJ80adK2G264oQNAampqfmZmZti6devC6hKfkgMREWkwOTk5Qenp6dHPPPPMxv/+978tSrePGDHiuNmzZ8eWro8ZMybh2WefbV5UVMTVV1/doXfv3j0TExOT//d//7cVwFtvvRVzwgknJI4aNapLUlJSL4Azzjija69evXp269at1wMPPNCqdF8PPfRQq4SEhN6pqalJ48aN61z6zX/r1q0hw4YN69q7d++evXv37vnBBx8cUz7e7OzsoFWrVkWdeOKJ+eXL/vOf/8SmpKT06NmzZ/KQIUMSN2/eHALw9ttvR/fo0SO5R48eyT179kzOzs4Omjp16rHp6enRPXr0SL777rtbl9/Xueeem9esWbOSsttKSkr48ssvYy677LJsgMsvv3znm2++2dz3+ZtffvnlOwEuu+yy7AULFsSUlHgvHz58+O7nnnuuBXWgWxlFRAQuv7wjy5dHHdZ99u69l5kzN1dX5YUXXmh+yimn5KSkpBQ0b968eP78+VFpaWl7x44du2v27Nktxo4dm7Nv3z774osvmj333HOb/vGPf7SKjY0tXr58+ar8/HwbNGhQj1GjRuUCLF269JhFixat6NGjx37fvje2adOmeM+ePdavX7/kiy++OHvfvn1BDzzwQLuFCxeubN68ecmQIUMSe/XqlQ9w9dVXd7zhhhu2DRs2bM+6devChg0b1v37779fUTbe+fPnH5OUlFQhMQA488wz94wbN251UFAQDz74YKt77rmn7VNPPbVl2rRpbadPn77prLPO+jknJycoKiqq5L777vtx2rRpbT799NOM2h7Obdu2hcTExBSHhoYCkJCQsH/btm1hvrKwLl267AcIDQ0lOjq6eNu2bSHt2rUrOuGEE37+29/+1g7YVtv3UnIgIiIN5uWXX2553XXX/QQwZsyYXbNmzWqZlpa294ILLsi5+eabO+Xn59ucOXNiU1NT86Kjo91HH33UbPXq1VFvvPFGC4C8vLzglStXRoSFhbmUlJSfSxMDgPvvv7/N22+/3RwgKysrdMWKFRFbt24NPeGEE/LatGlTDHD++ednr127NgLgiy++aLZu3brI0tfv2bMnODs7O6hFixa/fIP/8ccfQ+Pi4gor+ywbNmwIO++88zps3749dP/+/UEdO3YsABg8ePCeG2+8seNvf/vbXePHj8/u2rVrSWWvr4lzFW8sMTNXU1m7du2Ktm3bFlqX9wpYcmBmM4GRwE/OuQr3UZrZKcDrQOk1lVedc/cEKh4REalGDd/wAyErKyv4q6++arZ27drISZMmUVxcbGbmHnvssS1RUVFu8ODBea+++mqz2bNntxg/fvwuAOecTZs27YcxY8bklt3XW2+9FRMVFVVSdn3u3Lkx6enpq2NiYkpSU1OT8vPzgyo7iZZyzpGenr4qOjq6ykpRUVElBQUFlV6SnzRpUqfrrrsu66KLLsp56623Yu655572AH/5y1+yzjvvvJzXX389dsiQIT3fe++9tXU8VAC0bdu2KC8vL7iwsJDQ0FA2btwY1rp160Jf2f4NGzaEde3atbCwsJA9e/YEt27duhggPz8/KCIiok4JSSD7HDwL/LqGOvOcc319DyUGIiJNyKxZs1qMHj1659atW5f9+OOPy7KyspZ26NBh/wcffBANMG7cuF3PPvtsq2+//TZm9OjRuQBnnnlmzmOPPRZfUFBgAEuXLg3Pzc2tcC7bvXt3cGxsbHFMTEzJokWLIpYsWXIMwNChQ3/++uuvY7Zv3x5cWFjI66+//su1+LS0tNz777//l+v/CxYsiCy/3z59+uzbuHFjeGWfJy8vL7hTp06FAM8++2xc6fYVK1aEp6am5t93331Zffr0+Xn58uURsbGxxXv27Amuy/EKCgpi8ODBec8880wLgJkzZ8aNHDlyN8CIESN2z5w5Mw7gmWeeaXHiiSfmBQV5h2XlypXhVV0KqfK96lK5LpxznwO7ArV/ERE5sr3yyitxo0ePzi677dxzz80uvWvh/PPPz/32229j0tLSciMiIhzA9ddfv6NHjx77+vTp07N79+69rrzyys6FhYVWft9jxozJKSoqssTExOTbbrut/fHHH/8zQJcuXQqvv/76zEGDBvU86aSTkhITE/NjY2OLAZ588snNCxcuPCYxMTG5a9euvR599NH48vvt16/fvry8vODs7OwK58+pU6duHT9+fNcBAwYkxcXF/XLLxN///vfW3bt375WUlJQcGRlZcsEFF+Skpqbmh4SEuKSkpEo7JA4YMCBpwoQJx3355ZfN2rRpkzJnzpxmANOmTdvyyCOPtO3UqVPv7OzskOuuu24HwHXXXbcjOzs7pFOnTr0feeSRtg888MCW0n198sknzUaOHJlT258LgFXXxOJX0SwN6O6ce8bM4oFo51yF2yzKvSYBeKuaywpzgC3AVuBG59yK8vV8da8CrgLo1KnTgE2bNtUqZhGRo4EZ1PJfdTX7sO+ccwPLbluyZMnG448/fseh7fnIk5OTExQbG1tSWFjIsGHDul166aU7Jk6cuLu2r7/77rtbx8TElNxwww2N/tjl5+fb4MGDk9LT01eXdmQsa8mSJa2OP/74hPLba9VyYGZ3An8CbvVtCgX+ffDhArAQ6OycOx54BHitqorOuSedcwOdcwPj4yskciIiIrV20003te/Ro0dyYmJir06dOhVcfPHFtU4MfK/fHh4eflCdCutbRkZG2H333fdjZYlBdWrbIfF8oB/eCR3n3FYzi6lbiP6cc7lllt8xsxlm1so51+gzMREROXI9+eSTW2quVbWoqCh37bXXHhGXzfv06VPQp0+fgrq+rrZ9DvY77/qDAzCzCgND1JWZtTUz8y2n+mLZeaj7FRERkUNT25aDl83sCaC5mV0JXA48Vd0LzOxF4BSglZltAe7EuxyBc+5x4ALgGjMrAvKBca62HSBEREQkYGqVHDjnHjCzM4FcIAm4wzn3YQ2vGV9D+aPAo7UNVEREROpHjcmBmQUD7zvnzgCqTQhERETkyFdjnwPnXDGw18xia6orIiJSF5s3bw4ZNWpUlw4dOvTp1atXz759+/Z4/vnnmx/MvtasWRPWvXv3XpVtLzs98sFOs1yTG264of0dd9zRpi6viYqK6lfZ9jFjxiSUDnbUEGrbIXEfsMzMnjaz6aWPQAYmIiJHt5KSEkaNGtVt6NChe7Zs2bJsxYoVq15++eXvN2/eXKfphWtS0/TIVQn01M+NWW2Tg7eB24HPge/KPERERA7Km2++GRMaGupuvvnm7aXbEhMT90+dOvUngL1799oFF1yQkJiYmNyzZ8/kN998Mwa8loABAwYkJScn90xOTu754YcfVnsHXWXTI2dlZYUOHTq0e+fOnXv//ve/71BaNyoqqt+UKVPap6Sk9Pj444+j582bFzVo0KCkXr169UxLS+u+adOmUIB77723ddeuXXslJiYmjxw58rjS169atSoyNTU1qUOHDn3uvffeX0Y+vOuuu9p07969V/fu3Xvdc889FUZELCkpYeLEiZ26du3a65RTTum2Y8eOBp0YsbYdEp8zszAg0bdpjXOu0lmpRETkCJWamlRh2+jRu7jllu3k5QVx+undK5RffPEOJk/eSWZmCOee29Wv7Jtv1lT3dsuWLYtMSUnZW1V56TwHa9euXblo0aKIs88+u/v69euXt2/fvmjevHlro6Ki3LJly8LHjx9/3PLly1dVtZ/y0yNPnz49buXKlVFLlixZGRkZWdKtW7feN95447Zu3boV5ufnB/Xu3Tv/H//4x9aCggIbPHhw0ttvv53Rvn37oqeeeqrFjTfeeOwrr7yycfr06W03bdq0LDIy0u3YseOXORIyMjIiFixYsGb37t3BPXv27H3TTTdt/+abbyL/85//xH333XernHMMGDCg5+mnn5530kkn/TLfwaxZs5pnZGSEr1mzZsWWLVtC+/Tp0+vSSy9tsNv7a5Uc+IY6fg7YCBjQ0cwu8c2fICIicsgmTJjQ6ZtvvokODQ11y5cvX7VgwYLoP/7xjz+BN6dB+/bt9y9btiyiW7du+3/3u991XrlyZWRQUBCbNm2qdCKk6qSlpeXGxcUVA3Tr1m3f+vXrw7t161YYHBzMpZdemg3epE7r1q2LPO200xLB+3YfHx9fCJCUlJR//vnndznnnHN2X3TRRb+MsHjWWWftjoyMdJGRkUUtW7Ys3LJlS8hnn30WffbZZ+9u1qxZCcCIESOyP/3005iyycHcuXNjfvvb3+4KCQkhISGh8MQTT8w7lGN5qGrbbDENOMs5twbAzBKBF4EBgQpMRETqWXXf9GNiSqotb9euqKaWgvL69OmTX3ZWxFmzZv2QmZkZMnDgwJ7gTaFcmfvuu69N69atC+fMmbOhpKSEyMjIOp+LwsLCftl5cHCwK528KSwsrCQkxDs1OuesW7du+YsXL15d/vWffvrpunfffTfmtddea/73v/+9/bp165YDhIeHl90vRUVFdZnDqK4fI2Bq2+cgtDQxAHDOrcU3oJGIiMjBGDVqVF5BQYHdf//9v0yas2fPnl/OS2lpaXv+/e9/twTvW3xmZmZYSkrKvpycnOB27doVBgcHM2PGjLji4uJq3+dgpkcGSElJ2bdr166Qjz766BiAgoICS09PjyguLmb9+vVho0aNypsxY8aWvLy84JycnCr3f9ppp+155513mufl5QXl5uYGvfPOOy1OPfVUv5aBk08+Oe+VV15pWVRUxKZNm0K/+uqrQ5qi4FDVtuUg3cyeBmb51i9CHRJFROQQBAUF8eabb66/9tprO06fPr1ty5Yti6KioorvuuuuLQA333zzTxMmTOicmJiYHBwczBNPPLExMjLSTZky5acxY8Z0fe2111qkpaXlRUZGVjsJUtnpkS+88MIdLVq0qD6b8ImIiHAvvfTS+smTJ3fKy8sLLi4utmuuuWZbnz59Ci688MIueXl5wc45u/rqq7e1atWqyn2mpaXtvfDCC3f279+/J8CECRO2l72k4Nu2++OPP26WlJTUq0uXLvtSU1Mb9LJCrZo7zCwcuBZIw+tz8DkwwzlX58kcDtXAgQNdenp6fb+tiEiD0ZTNEihVTdlc25aDEOBh59yD8MuoiXXuACIiIiKNX237HHwMRJZZjwQ+OvzhiIiISEOrbXIQ4ZzbU7riW44KTEgiIiLSkGqbHPxsZv1LV8xsIN40yyIiInKUqW2fgynAK2a2FXBAe2BswKISERGRBlNty4GZDTKzts65b4EewGygCHgP2FAP8YmIiEg9q6nl4AngDN/yicBtwB+BvsCTwAWBC01EROqT2eEd9da52o2H86c//antnDlz4oKCglxQUBAzZszYdNppp/18OGORuqkpOQh2zu3yLY8FnnTOzQHmmNniwIYmIiJHu48++uiY999/v/myZctWRkZGuszMzJCCgoKDHke4sLCQ0FAN4HuoauqQGGxmpQnE6cAnZcoadDpJEZGjWSMaZj+gfvzxx9CWLVsWRUZGOoB27doVJSQkFM6dOzeqX79+PZKSkpL79OnTMzs7O6iqKZynT58eN3z48ONOO+20bkOHDk0EuP3229v07t27Z2JiYvL111/fHiA3NzfolFNO6ZaUlJTcvXv3Xk899VSLqiNr2mo6wb8IzDWzHXh3J8wDMLNuQE6AYxMRkaPceeedl/vXv/61fUJCQu+0tLTc8ePH7zr99NN/vuiii7q+8MIL608++eS9u3btCoqOji65995720DFKZwBFi5cGL106dIVbdq0KX711VebZWRkRCxdunSVc44zzjij27vvvhu9bdu2kLZt2xZ+9tlnGQA7d+6s83wLTUW1LQfOufuA/wGeBdLcgbGWg/D6HoiIiBy02NjYkuXLl6989NFHN8XHxxddcsklXadNmxbfunXrwpNPPnkvQMuWLUtCQ0NZsGBB9MSJE3eC/xTOAEOHDs1t06ZNMcB7773X7PPPP2+WnJyc3KtXr+T169dHrF69OqJ///758+bNa3bNNdcc+95770WXTtksFdV4acA591Ul29YGJhwREWlqQkJCGDlyZN7IkSPzUlJS8h9//PF4M6swm0R1cwFFRUWVlK03ZcqUzJtuuqnCvBELFy5cOWfOnNipU6ce+9FHH+U+8MADmYftgxxFajsIkoiIyGG3ZMmS8GXLlv0yV8+iRYsiu3fvvm/btm1hc+fOjQLIzs4OKiwsrHIK5/L7HD58eO6sWbNa5eTkBAFs2LAh9McffwzZuHFjaExMTMkf/vCHXVOmTNm2ePFijfRbBXUqFBERoPa3Hh5Oubm5wZMnT+6Um5sbHBwc7BISEgqee+65TWvXrt0xefLkTvv27QuKiIgo+fzzz9dWNYVz+X2OHj06d8WKFRGDBg3qAV6rwgsvvLBh9erV4bfeemuHoKAgQkJC3IwZMzbV9+c9UtRqyubGRFM2i0hTUHaaZk3ZLIFS1ZTNuqwgIiIifpQciIiIiB8lByIiTVdJSUlJExluScrz/exLKitTciAi0nQt3759e6wShKanpKTEtm/fHgssr6xcdyuIiDRRRUVFV2RlZf0rKyurN/qy2NSUAMuLioquqKxQyYGISBM1YMCAn4BzGjoOaXwClima2Uwz+8nMKm2yMM90M8sws6Vm1j9QsYiIiEjtBbIZ6Vng19WUDwe6+x5XAY8FMBYRERGppYAlB865z4Fd1VQ5F3jeeb4CmptZu0DFIyIiIrXTkB1QjgU2l1nf4ttWgZldZWbpZpa+ffv2eglORESkqWrI5KCyW2cqHSDUOfekc26gc25gfHx8gMMSERFp2hoyOdgCdCyz3gHY2kCxiIiIiE9DJgdvABN9dy0MBnKcc5pXW0REpIEFbJwDM3sROAVoZWZbgDuBUADn3OPAO8DZQAawF7gsULGIiIhI7QUsOXDOja+h3AHXBur9RURE5OBouEwRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgMRERHxo+RARERE/Cg5EBERET8BTQ7M7NdmtsbMMszslkrKLzWz7Wa22Pe4IpDxiIgcsdavh9zcho5CmoiAJQdmFgz8ExgOJAPjzSy5kqqznXN9fY9/BSoeEZEjlVEC3brBgw82dCjSRIQEcN+pQIZz7nsAM3sJOBdYGcD3rN4jj8CLL1bc/sUXYAb33w+vv+5fFhkJH3/sLd95J3z4oX95XBy8+aa3fPPNMH++f3nHjjB7trc8aRIsXOhfnpgIzz7rLV9+Oaxe7V/ety/MmOEtjxsHP/zgXz5kCDzwgLd8zjmwY4d/+RlnwD33HFjeu9e//Jxz4JZbDuyrvHHjYPJk73VnnFGx/PLL4YorvPc955yK5ZMmwYUXenGPG1ex/Oab4bzzYM0auOyyiuV33QVnnQWLFsG111Ysv/9+GDrU+xnedFPF8kcegQEDvJ/bnXdWLH/6aejZ0/u5339/xfIXX4TOnb3nRx6pWP766xAf7+3n6acrln/4IRxzjH739LtXsbyG371OvAh4v3sL+Ie3MTq64n5EAiCQycGxwOYy61uAEyqpN8bMfgWsBa53zm0uX8HMrgKuAujUqdPBRxQeXv0fV2XlERH+y+XLjzmm+vKoqAPLkZF1L4+M9K9b1/Ky8UdHQ1BQ9eXlhYV5z2YHVx4a6j0HBR1ceYjvVzQ4+ODKg4MP1DuY8tLjFRZ2cOWl9Lun3706lpdw4HcrggJvuezPXCSAzDkXmB2b/QYY5py7wrc+AUh1zv2xTJ04YI9zrsDMfg/81jl3WnX7HThwoEtPTw9IzCIijYUZlP57vs3+wl+YCrt2QYsWB7k/+845N/AwhihHsUB2SNwCdCyz3gHYWraCc26nc86XEvMUMCCA8YiIHJH+wlRv4SATA5G6CmRy8C3Q3cy6mFkYMA54o2wFM2tXZvUcYFUA4xEREZFaCFifA+dckZlNAt4HgoGZzrkVZnYPkO6cewOYbGbnAEXALuDSQMUjInKk2klL4kad1NBhSBMSyA6JOOfeAd4pt+2OMsu3ArcGMgYRkaPCoXTGFqkjjZAoItKYOUcsORAb29CRSBMS0JYDERE5BB98AOnphFCs5EDqlZIDETnqlb0tsDG9Z3V1wiiAYcMObFByIPVIlxVEROrDmjWQmgq33lpxNEqAWbNg3bpfVscy27+8V68AByhygJIDEZH68O9/w7ffwt/+5g2b/OWXB8o2bYKJE70hrQE+/5znucT/9RrjQOqRkgMRkfqQn+89JyV5z0OGMAnffB1l58XYvRumT/eWR48+sD0+PvAxivgoORARCbTiYpg2zVt+9dVfNj/CZMjM9FoUSv35z7BuHZ9xMsyZA3/9K//id9C6dT0HLU2ZkgMRkUD64QdvdshSycneSb9U+/ZegpCQ4K3/97+wdClZtPXWb7mFK9Fs9lK/dLeCiEgAhFEAPfv6dz58/HHvefRoWLHiQCfDl1/2puDu0AE2bABgN83rOWKRA9RyICISACcz90BiMGGC1+HwyisPVEhO5imuOLB+++3wm994y/fdx7X8s/6CFfrRaMUAABIkSURBVClHLQciIgHQjkxvYd48SEurtM5DXM+VZ/0Azz+PtW2DK3GEPfQ39t8WRsnUegxWpBwlByIiAdCGbd5C375V1llFMrz//oENZhQSFuDIRGqmywoiIgHQhm0QFQXR0Q0dikidKTkQEQmAtmRBmzYNHYbIQdFlBRGRw805LuI/kBXZ0JGIHBS1HIiIHG4ffug99+jRsHGIHCQlByIih9tf/+o9f/RRw8YhcpCUHIiIHE7vvAOffeYtt2zZoKGIHCwlByJSJ2YNHUH1ysdX13gP6vNlZsILL3jLd9wBwAjeqnRfhxqfSH1Qh0QRkUOxb583PwLAxRd7z0OG8M6CEQ0Xk8ghUsuBiMjBcA4++QTOPrti2Zgx9R+PyGGklgMRabrWr/eGN96xA0aOrP7ugpISuO8+2LQJnn7av+y44yAjA555xruscPrpgY1bJMDMOdfQMdTJwIEDXXp6ekOHIdJkmXlfmhur8vGVXtN3K1fBY495kyHFxEBKCtx1l/+LR4zgsrfH8My87pCTA126eJcMmjf3OhqOKHep4NZbYcoUaNUKgvwbYn9533LHqmx8ZeuUbi//XNlnOhhm9p1zbuCh7UWaCiUHIlInR1pyEGqF/In7uTfkbigq8q8cE+Od9N99F/7yl8p3GB3tJQFTp0JYGMyY4U2k1KWLt15NHKDkQI5MSg5EpE6OtOTgHruDO/izN5TxpZfCySdDu3awZw8MGeL/jT83lymxM/lHn5nwwAOwZAncfLNXlpgIM2fCSSfVOg5QciBHJiUHIlInNZ6oyp/56tkv8f30E9x4I8yaxaucz2j3at1eD97Chx96CcQpp0BI7btpKTmQI5k6JB5F6uMbXVXvUdl13qpiqeqfZk1lh6Kyf7Q1xVHZP/Dy67X93LV5z8r2XZXKzr8V4thfSLuwHXRgC98uiySmT2fyXEyZ1znc62/y63PDeG/fqQD8LeJOBvAdZ46M4Lm3WrKYvjy0+DQm953L9PM+hdRU/slm/mX7ac1PDCSd9mSylXYEU0yb5vsp2L2X+aRx+vx7sLSTajzBVXUyrOo4VfezBEhkDb+z+VzPQ3QjgyeYzJ+4n4Jyx6o2P0cLMuCsSutXt1xZzLVZr2z74UwQRGpLLQeHWUP+ASs5qNqRnhy0IYuJPM9W2tOGbUy7MQuio3n4rl0cx/d04geOb7/DG5Fv9252btlLHLsqvsGJJ0JqKp8+vITjWUJLsgH4iXhaxwPbt3v1+vUje9EGWrD7wGtDQ6GwEIAfaU8OsSwlhfZsJYNuFBDONZPD+Gz6Ek5hLgAvMo7xy6ZC794BTw6OYz0fczoJbAKgkBDO4Q3eY3iFfVSXHNQlCahNclBToleZ6t7jYKnlQOpCLQcih8O+fb4m52BwvvXIOs7I5xzDeZc1JMH7GVxCJgNJ51xepyNb/Os+FAzFxVwHZNKWQkK994yLg2bN+HjLsfx2dDF3vHo8S0nhtf/kM//Cf5K2dDEsW0ZrOvEBZzHu5s5c+PfjuYTnGHZ6S254aRDPcim7FrYkzkrowWpW3v1/nH3nQN7ZexZkZJDU01hLUqUf4ZqH4dTp0I6tbP2fBxkzbTr0eQkGDaIfjwP9az4Oe/cSTTH5RMKSFd7og4MHQ2wsmBFCISxbDT/+yCVkwcwS/peVXMs/iWQfzzOBGfyB7zmO7bSu289ARAC1HBx2ajmoOZYjuuUgJ4eTmi9nJG9x6yWZrH1uAYmsq/xNW7SA006DwkI+euNnzrg+haceyqMfixgYtQqaNfO+jR93HPz8Myxc6N1LX0YJxrsMZxH9+IFOfMqp7CGazJK2sGwZqcfv41tSK3yOyj7joX47rk1XgvKvi7Od7Lz/aa9z3/bt3vG4917o3x8KCqCoiFFxX/DmE5l8c/W/SG2RAdnZVR/PE05g63tLaE9mheIMunIur7OSXlUHWEWcajkQ8aeWAwmsrCzYuNG7D3znTigpoR2diWMnvLIaBg2C1q0hPBzWruVaPqYbGXBblHfSLCmBV17xlnfs8O4zHz++7t/Kq9COrTB3nTewzb59kJAA+/fzZ76Cq36CFSvIIIufOQaa/wA5OXyBd9LmjeZ0Yq93kj/zTIiL450nN3P2Fcd6/8W/+w4WLYLISFoRAg89xJXAUvrAr37l9ZifO9drqvclvLP5LZm0Y8pLJ9JnXDLZtOBHOlQM3ICUFL49LEchcHYR5/X2v+IKbo/7J39e+KB3hwB4LS1FRbwJcDW0owOcfz5ERHDjjC4ksYYrb2/nJRL//Kc31sCXX5JFW9rfdhkMHkzPc7qxat5Ofjd0Dc9wGU6DvoocFgFtOTCzXwMPA8HAv5xzfytXHg48DwwAdgJjnXMbq9unWg6q1tayyJr1kXdCWrECdu2C+HjvxPub30Dv3tCxo/dP9iCZgStxUFzsndT27YNt20jpuZ+l3+73moD37uWecSu4o9nDkJtb806DguCYYyAvD4ACwggPKvL/Fh0b6z3n5EBEBBx7rHf/eVycN0jNmWd6Q9Yec4z/vktKYOdOOrbex+ZPMmDlSh6alMH1Q75h/YIsuvJ91XG1bAlRUbBlCytIpteVJ0FeHr95aTTzSSPTtatbn4PdOYQ2j6KI0Ip1nIOSEiwk+JfVQ+6Q2AhaDiq02GRmedMYL1oE69bBzp2cvOAvzF3dlvAeCRS48ArxlVdTh8TaUMuBSPUClhyYWTCwFjgT2AJ8C4x3zq0sU+cPQIpz7vdmNg443zk3trr9HlJysHu31zQJ3omkb1+vabOgwDsJhIR4J6Gyj5gYr+k3KOjAIzi44nJMDAQFEddsPzs35EF+vrfdzP+1pY/wcG8AlYICL669e70Tme8kUevnXbu86WGXLKF4wVcEU+Ltv3dv77Nu3Oh9Ky4VHu417fbv740QVzql7O7d3mdo2dIbKKaw0HtkZnqJxvbtsGsXP/13Pq3ZXrvjPXy4d9I+5hjvOMTFQWQkt529iH1E8OBLx3oniK1bvfqJiaReP4RvGYQrKvFGsps3z4tp9GivzhtveIPW7N7t3aq2Y4cX244dXnnbtt6xCQ/3jk1RkZfAlLGfUMIGHs9r6ceygS5c/59U7xidcYaXoBQVEXPW4HI9+xu2Q2J5R2xyEMAOiXWh5ECkeoFMDk4E7nLODfOt3wrgnPtrmTrv++p8aWYhQBYQ76oJ6qCTg3ff9Z8gpXNn/5PmkSw8HPr25a9fn8qtX50HAwd6SUup/fvh66+9k/Bnn3n3bW/YUOH6dpWCg71m/OhoZmf9irG3J3kJU+mjdWvGTIxizhthXpLVqhUD+xaSXjKg0v+Kh73PQVERzJ8Pb70Fmzd78W7Z4m1PTYWEBH53fQxPz46BIUMI7xhPgQs/uD4HVH1SKf+6yvan5KDyz6zkoOb4SpcPlpIDqYtAJgcXAL92zl3hW58AnOCcm1SmznJfnS2+9fW+OjvK7esq4CrfahKw5iDDagXsqLFW43Kkxax4A0vxBtbRHG9n51x8IIORo0cgOyRWliuXz0RqUwfn3JPAk4cckFn6kZY5H2kxK97AUryBpXhFPIHs2rsF6FhmvQOwtao6vssKsVDZyC0iIiJSXwKZHHwLdDezLmYWBowD3ihX5w3gEt/yBcAn1fU3EBERkcAL2GUF51yRmU0C3se7lXGmc26Fmd0DpDvn3gCeBmaZWQZei8G4QMXjc8iXJhrAkRaz4g0sxRtYileEAHZIFBERkSOThhMTERERP0oORERExE+TSQ7M7NdmtsbMMszsloaOp5SZbTSzZWa22MzSfdtamtmHZrbO99zCt93MbLrvMyw1s1pMcXfI8c00s598Y1KUbqtzfGZ2ia/+OjO7pLL3CmC8d5nZj75jvNjMzi5Tdqsv3jVmNqzM9nr5fTGzjmb2qZmtMrMVZnadb3ujPMbVxNuYj3GEmX1jZkt8Md/t297FzL72Ha/Zvo7TmFm4bz3DV55Q02epp3ifNbMNZY5xX9/2Bv+7k6OQc+6of+B1iFwPHAeEAUuA5IaOyxfbRqBVuW1/B27xLd8C3O9bPht4F298iMHA1/UQ36/w5tldfrDxAS2B733PLXzLLeox3ruAGyupm+z7XQgHuvh+R4Lr8/cFaAf09y3H4A05ntxYj3E18TbmY2xAtG85FPjad+xeBsb5tj8OXONb/gPwuG95HDC7us9Sj/E+C1xQSf0G/7vT4+h7NJWWg1Qgwzn3vXNuP/AScG4Dx1Sdc4HnfMvPAeeV2f6883wFNDezdoEMxDn3ORXHnqhrfMOAD51zu5xz2cCHwK/rMd6qnAu85JwrcM5tADLwflfq7ffFOZfpnFvoW84DVgHH0kiPcTXxVqUxHGPnnNvjWw31PRxwGvB/vu3lj3Hpsf8/4HQzs2o+S33FW5UG/7uTo09TSQ6OBTaXWd9C9f/Q6pMDPjCz78wbJhqgjXMuE7x/xkBr3/bG8jnqGl9jiHuSr8l1ZmkTfTVxNUi8vubrfnjfFBv9MS4XLzTiY2xmwWa2GPgJ7yS5HtjtnCuq5P1/ic1XngPE1WfM5eN1zpUe4/t8x/gh82a19Yu3XFyN4e9OjlBNJTmo1TDNDeQk51x/YDhwrZn9qpq6jflzQNXxNXTcjwFdgb5AJjDNt73RxGtm0cAcYIpzrrp5rhtFzJXE26iPsXOu2DnXF2+k1lSgZzXv3+Axl4/XzHoDtwI9gEF4lwr+5Kve4PHK0aepJAe1Gcq5QTjntvqefwL+i/ePa1vp5QLf80++6o3lc9Q1vgaN2zm3zffPtgR4igNNwY0iXjMLxTvRvuCce9W3udEe48ribezHuJRzbjfwGd61+ebmDdte/v2rGta93mMuE++vfZd0nHOuAHiGRnqM5ejQVJKD2gzlXO/M7BgziyldBs4CluM/rPQlwOu+5TeAib7eyYOBnNKm53pW1/jeB84ysxa+5uazfNvqRbl+GefjHePSeMf5eqd3AboD31CPvy++a9lPA6uccw+WKWqUx7iqeBv5MY43s+a+5UjgDLy+Ep/iDdsOFY9xZcO6V/VZ6iPe1WWSRcPrH1H2GDe6vzs5wjVUT8j6fuD16F2Ld61xakPH44vpOLzez0uAFaVx4V3f/BhY53tu6dtuwD99n2EZMLAeYnwRr5m4EO+byO8OJj7gcrwOXBnAZfUc7yxfPEvx/pG2K1N/qi/eNcDw+v59AdLwmnqXAot9j7Mb6zGuJt7GfIxTgEW+2JYDd5T5+/vGd7xeAcJ92yN86xm+8uNq+iz1FO8nvmO8HPg3B+5oaPC/Oz2OvoeGTxYRERE/TeWygoiIiNSSkgMRERHxo+RARERE/Cg5EBERET9KDkRERMSPkgNpMsysuMyMdouthpkAzez3ZjbxMLzvRjNrdRCvG2bebIctzOydQ41DRKS2QmquInLUyHfekLS14px7PJDB1MJQvIF6fgV80cCxiEgTouRAmjwz2wjMBk71bbrQOZdhZncBe5xzD5jZZOD3QBGw0jk3zsxaAjPxBtPZC1zlnFtqZnF4gzHF4w2iY2Xe62JgMt40xV8Df3DOFZeLZyzeOPrH4c241wbINbMTnHPnBOIYiIiUpcsK0pRElrusMLZMWa5zLhV4FPhHJa+9BejnnEvBSxIA7gYW+bbdBjzv234nMN851w9vtMBOAGbWExiLN9lWX6AYuKj8GznnZgP9geXOuT54I+L1U2IgIvVFLQfSlFR3WeHFMs8PVVK+FHjBzF4DXvNtSwPGADjnPjGzODOLxbsMMNq3/W0zy/bVPx0YAHzrDY9PJAcmVCqvO95wuABRzrm8Wnw+EZHDQsmBiMdVsVxqBN5J/xzgdjPrRfVT4la2DwOec87dWl0gZpYOtAJCzGwl0M7MFgN/dM7Nq/5jiIgcOl1WEPGMLfP8ZdkCMwsCOjrnPgVuBpoD0cDn+C4LmNkpwA7nXG657cOBFr5dfQxcYGatfWUtzaxz+UCccwOBt/H6G/wdb1KivkoMRKS+qOVAmpJI3zfwUu8550pvZww3s6/xEubx5V4XDPzbd8nAgIecc7t9HRafMbOleB0SS6f5vRt40cwWAnOBHwCccyvN7P8BH/gSjkLgWmBTJbH2x+u4+AfgwUrKRUQCRrMySpPnu1thoHNuR0PHIiLSGOiygoiIiPhRy4GIiIj4UcuBiIiI+FFyICIiIn6UHIiIiIgfJQciIiLiR8mBiIiI+Pn/DKoCYScpSAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 winner agent reward: 0.5000000074505806\n",
      "Episode 2 winner agent reward: 0.5000000074505806\n",
      "Episode 3 winner agent reward: 0.5000000074505806\n",
      "Episode 4 winner agent reward: 0.6000000089406967\n",
      "Episode 5 winner agent reward: 0.5000000074505806\n"
     ]
    }
   ],
   "source": [
    "watch_game(maddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
